# src/ai_agent/q_learning.py
import numpy as np
import random
import os
from src.python.environment.maze import MazeEnv


class QLearningAgent:
    def __init__(self, env, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995):
        self.env = env
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.action_size = 4  # up, down, left, right
        self.q_table = {}  # {(row, col): [Q_up, Q_down, Q_left, Q_right]}
        self.episodes_trained = 0  # Compteur d'√©pisodes total

    def ensure_state(self, state):
        if state not in self.q_table:
            self.q_table[state] = np.zeros(self.action_size)

    def choose_action(self, state):
        self.ensure_state(state)
        if random.random() < self.epsilon:
            return random.randint(0, self.action_size - 1)
        return int(np.argmax(self.q_table[state]))

    def update(self, state, action, reward, next_state, done):
        self.ensure_state(state)
        self.ensure_state(next_state)

        q_predict = self.q_table[state][action]
        q_target = reward
        if not done:
            q_target += self.gamma * np.max(self.q_table[next_state])
        self.q_table[state][action] += self.alpha * (q_target - q_predict)

    def decay_epsilon(self):
        """R√©duit epsilon progressivement"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def reset_exploration(self, epsilon=0.3):
        """R√©active l'exploration si l'agent est bloqu√©"""
        old_epsilon = self.epsilon
        self.epsilon = epsilon
        print(f"üîÑ Exploration r√©activ√©e: {old_epsilon:.4f} ‚Üí {epsilon:.4f}")

    def save(self, filename, best_reward=None):
        """Sauvegarde compl√®te de l'agent avec toutes ses informations"""
        save_data = {
            'q_table': self.q_table,
            'best_reward': best_reward,
            'episodes_trained': self.episodes_trained,
            'hyperparameters': {
                'alpha': self.alpha,
                'gamma': self.gamma,
                'epsilon': self.epsilon,
                'epsilon_min': self.epsilon_min,
                'epsilon_decay': self.epsilon_decay
            }
        }
        
        # Cr√©er le dossier si n√©cessaire
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, 'wb') as f:
            np.save(f, save_data, allow_pickle=True)
        print(f"üíæ Mod√®le sauvegard√©: {filename}")
        print(f"   ‚îî‚îÄ Episodes: {self.episodes_trained}, Epsilon: {self.epsilon:.4f}, Best reward: {best_reward}")
    
    @classmethod        
    def load(cls, filename, env=None):
        """Charge un agent avec TOUTE sa m√©moire"""
        try:
            with open(filename, 'rb') as f:
                saved_data = np.load(f, allow_pickle=True).item()
                
            agent = cls(env=env)
            
            # Restaurer la Q-table (LA M√âMOIRE!)
            agent.q_table = saved_data.get('q_table', {})
            
            # Restaurer le nombre d'√©pisodes
            agent.episodes_trained = saved_data.get('episodes_trained', 0)
            
            # Restaurer les hyperparam√®tres
            if 'hyperparameters' in saved_data:
                params = saved_data['hyperparameters']
                agent.alpha = params.get('alpha', agent.alpha)
                agent.gamma = params.get('gamma', agent.gamma)
                agent.epsilon = params.get('epsilon', agent.epsilon)
                agent.epsilon_min = params.get('epsilon_min', agent.epsilon_min)
                agent.epsilon_decay = params.get('epsilon_decay', agent.epsilon_decay)
            
            best_reward = saved_data.get('best_reward', 'Inconnu')
            
            print(f"‚úÖ Mod√®le charg√©: {filename}")
            print(f"   ‚îú‚îÄ Q-table: {len(agent.q_table)} √©tats connus")
            print(f"   ‚îú‚îÄ Episodes entra√Æn√©s: {agent.episodes_trained}")
            print(f"   ‚îú‚îÄ Epsilon actuel: {agent.epsilon:.4f}")
            print(f"   ‚îî‚îÄ Meilleur reward: {best_reward}")
            
            return agent, saved_data
            
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Aucun mod√®le trouv√©: {filename}")
            return None, None
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {e}")
            return None, None
        

def train(agent, env, episodes=50, max_steps=200, save_interval=100, model_filename="agent_model.npy"):
    """Entra√Ænement avec sauvegarde automatique de la m√©moire"""
    
    # Charger le meilleur reward pr√©c√©dent
    best_reward = float('-inf')
    if os.path.exists(model_filename):
        try:
            with open(model_filename, "rb") as f:
                saved_data = np.load(f, allow_pickle=True).item()
                best_reward = saved_data.get('best_reward', float('-inf'))
        except:
            pass

    print(f"\n{'='*60}")
    print(f"üéì D√âBUT DE L'ENTRA√éNEMENT")
    print(f"{'='*60}")
    print(f"Episodes pr√©vus: {episodes}")
    print(f"Episodes d√©j√† effectu√©s: {agent.episodes_trained}")
    print(f"Epsilon initial: {agent.epsilon:.4f}")
    print(f"Meilleur reward actuel: {best_reward}")
    print(f"{'='*60}\n")
    
    for ep in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        steps = 0

        # EXPLORATION DE L'√âPISODE
        while not done and steps < max_steps:
            action = agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)
            agent.update(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            steps += 1

        # MISE √Ä JOUR POST-√âPISODE
        agent.episodes_trained += 1
        agent.decay_epsilon()  # R√©duire l'exploration progressivement

        # Affichage p√©riodique
        if ep % 10 == 0 or ep == episodes - 1:
            print(f"Episode {ep:4d}/{episodes} | Reward: {total_reward:7.2f} | Steps: {steps:3d} | Epsilon: {agent.epsilon:.4f} | √âtats connus: {len(agent.q_table)}")
        
        # üî• SAUVEGARDE SI NOUVEAU RECORD
        if total_reward > best_reward:
            best_reward = total_reward
            print(f"   üéâ NOUVEAU RECORD! {total_reward:.2f} ‚Üí Sauvegarde automatique")
            agent.save(model_filename, best_reward=best_reward)
        
        # üíæ CHECKPOINT P√âRIODIQUE (toutes les N it√©rations)
        if (ep + 1) % save_interval == 0:
            checkpoint_file = f"checkpoints/agent_ep{agent.episodes_trained}.npy"
            agent.save(checkpoint_file, best_reward=best_reward)
    
    # SAUVEGARDE FINALE
    print(f"\n{'='*60}")
    print("üíæ Sauvegarde finale...")
    agent.save(model_filename, best_reward=best_reward)
    print(f"{'='*60}")


def test(agent, env, max_steps=200, render=True):
    """Test de l'agent (MODE EXPLOITATION PURE)"""
    # Sauvegarder l'epsilon actuel
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0  # D√©sactiver l'exploration pour le test
    
    state = env.reset()
    total_reward = 0
    steps = 0
    
    if render:
        print(f"\n{'='*60}")
        print("üß™ TEST DE L'AGENT (pas d'exploration)")
        print(f"{'='*60}")
        env.render()
        print("-" * 40)
    
    for step in range(max_steps):
        agent.ensure_state(state)
        action = int(np.argmax(agent.q_table[state]))  # Toujours la meilleure action
        next_state, reward, done, _ = env.step(action)
        
        total_reward += reward
        state = next_state
        steps = step + 1
        
        if render:
            env.render()
        
        if done:
            if reward > 0:
                print(f"\n‚úÖ SUCC√àS en {steps} √©tapes!")
            else:
                print(f"\n‚ùå √âCHEC apr√®s {steps} √©tapes")
            print(f"   Reward total: {total_reward:.2f}")
            break
    else:
        print(f"\n‚è±Ô∏è  Timeout ({max_steps} √©tapes max)")
        print(f"   Reward total: {total_reward:.2f}")
    
    # Restaurer epsilon
    agent.epsilon = original_epsilon
    
    return total_reward, steps


def generate_maze(width=31, height=31):
    """G√©n√®re un labyrinthe al√©atoire avec l'algorithme de backtracking r√©cursif"""
    maze = np.ones((height, width), dtype=int)

    def carve(r, c):
        random.seed(42)
        dirs = [(2, 0), (-2, 0), (0, 2), (0, -2)]
        random.shuffle(dirs)
        for dr, dc in dirs:
            nr, nc = r + dr, c + dc
            if 1 <= nr < height-1 and 1 <= nc < width-1 and maze[nr, nc] == 1:
                maze[nr-dr//2, nc-dc//2] = 0
                maze[nr, nc] = 0
                carve(nr, nc)

    maze[1, 1] = 0
    carve(1, 1)
    random.seed()  # R√©initialiser le seed global
    return maze


if __name__ == "__main__":
    # G√©n√©rer ou charger un labyrinthe
    maze = generate_maze(21, 21)  # Plus petit pour un apprentissage plus rapide
    env = MazeEnv(maze, start=(1, 1), goal=(19, 19))

    model_file = "agent_model.npy"

    # 1Ô∏è‚É£ TENTATIVE DE CHARGEMENT D'UN MOD√àLE EXISTANT
    loaded_agent, saved_data = QLearningAgent.load(model_file, env=env)

    if loaded_agent is not None:
        # Mod√®le trouv√© : proposer de continuer ou tester
        print("\nü§î Options:")
        print("  1. Continuer l'entra√Ænement (l'agent garde sa m√©moire)")
        print("  2. Tester le mod√®le actuel")
        print("  3. Recommencer de z√©ro (PERTE DE LA M√âMOIRE)")

        choice = input("\nVotre choix (1/2/3): ").strip()

        if choice == "1":
            # CONTINUER L'APPRENTISSAGE
            nb_episodes = int(input("Nombre d'√©pisodes suppl√©mentaires: "))
            loaded_agent.reset_exploration(epsilon=0.3)  # R√©activer un peu l'exploration
            train(loaded_agent, env, episodes=nb_episodes, max_steps=500, model_filename=model_file)
            agent = loaded_agent

        elif choice == "2":
            # JUSTE TESTER
            agent = loaded_agent

        else:
            # RECOMMENCER
            print("\n‚ö†Ô∏è  Vous allez perdre toute la m√©moire de l'agent!")
            confirm = input("Confirmer (oui/non): ").strip().lower()
            if confirm == "oui":
                agent = QLearningAgent(env)
                train(agent, env, episodes=5000, max_steps=500, model_filename=model_file)
            else:
                print("Annul√©.")
                exit()
    else:
        # 2Ô∏è‚É£ PAS DE MOD√àLE : CR√âER UN NOUVEL AGENT
        print("\nüÜï Cr√©ation d'un nouvel agent")
        agent = QLearningAgent(env)
        train(agent, env, episodes=5000, max_steps=500, model_filename=model_file)

    # 3Ô∏è‚É£ TEST FINAL
    print("\n" + "="*60)
    print("üéØ TEST FINAL")
    print("="*60)
    test(agent, env, max_steps=500, render=True)
